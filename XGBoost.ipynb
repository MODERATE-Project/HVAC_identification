{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6eae871",
   "metadata": {},
   "source": [
    "# Train with real data from Fluvius"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44226fec",
   "metadata": {},
   "source": [
    "## Read raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1f8bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "directory = Path.cwd()\n",
    "time_series_Consumption=pd.read_pickle(os.path.join(directory,'all_time_series_Consumption.pkl'))\n",
    "labels_df=pd.read_pickle(os.path.join(directory,'labels_df.pkl'))\n",
    "time_series_Consumption.set_index('Datetime', inplace=True)\n",
    "time_series_Consumption=time_series_Consumption.resample('h').sum()\n",
    "\n",
    "#features_to_plot = [\n",
    "#    ('mean', \"Mean\"),\n",
    "#    ('std', \"Standard Deviation\"),\n",
    "#    ('median', \"Median\"),\n",
    "#    ('skew', \"Skewness\"),\n",
    "#    ('sum', \"Sum\"),\n",
    "#    ('min', \"Minimum\"),\n",
    "#    ('max', \"Maximum\")]\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dfb0fc5",
   "metadata": {},
   "source": [
    "## Extract features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc687cb1",
   "metadata": {},
   "source": [
    "### Fuction for extarcting features\n",
    "Featue engineering has been done in a seperate excerceis with a collection of scripts in the file named development.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee45965",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Define a function to extract weekly and monthly features with prefixed row names and a single 'feature' column\n",
    "def extract_time_features(df, freq, prefix):\n",
    "    # Calculate statistics for each period individually\n",
    "    mean = df.resample(freq).mean()\n",
    "    mean.index = [f\"{prefix}_mean_{i}\" for i in range(len(mean))]\n",
    "    \n",
    "    std = df.resample(freq).std()\n",
    "    std.index = [f\"{prefix}_std_{i}\" for i in range(len(std))]\n",
    "    \n",
    "    skew = df.resample(freq).apply(pd.Series.skew)\n",
    "    skew.index = [f\"{prefix}_skew_{i}\" for i in range(len(skew))]\n",
    "    \n",
    "    sum_ = df.resample(freq).min()\n",
    "    sum_.index = [f\"{prefix}_sum_{i}\" for i in range(len(sum_))]\n",
    "    \n",
    "    max_ = df.resample(freq).max()\n",
    "    max_.index = [f\"{prefix}_max_{i}\" for i in range(len(max_))]\n",
    "    \n",
    "    # Concatenate all features\n",
    "    features = pd.concat([mean, std, sum_, skew, max_], axis=0) #min_,\n",
    "    \n",
    "    # Reset the index to have 'feature' as a column and IDs as columns\n",
    "    features = features.reset_index()\n",
    "    features = features.rename(columns={'index': 'feature'})\n",
    "    \n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365e6e6e",
   "metadata": {},
   "source": [
    "### Train, test and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aeb7ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Extract weekly and monthly features for each ID\n",
    "weekly_features = extract_time_features(time_series_Consumption, 'W', 'W')\n",
    "monthly_features = extract_time_features(time_series_Consumption, 'ME', 'M')\n",
    "Annual_features = extract_time_features(time_series_Consumption, 'YE', 'A')\n",
    "# Combine weekly and monthly features into a single DataFrame\n",
    "combined_features = pd.concat([weekly_features, monthly_features,Annual_features], axis=0).reset_index(drop=True)\n",
    "\n",
    "new_column_names = combined_features['feature'].values\n",
    "combined_features_temp= combined_features.drop(columns=['feature']).transpose()\n",
    "combined_features_temp.columns = new_column_names\n",
    "combined_features_temp=combined_features_temp.reset_index().rename(columns={'index': 'ID'})\n",
    "combined_features_temp.ID=combined_features_temp.ID.astype(int)\n",
    "# Step 1: Merge features and labels\n",
    "data = pd.merge(combined_features_temp, labels_df[['ID', 'Category']], on='ID')\n",
    "\n",
    "# Step 2: Separate features (X) and target (y)\n",
    "X = data.drop(columns=['Category','ID'])\n",
    "y = data['Category']\n",
    "\n",
    "# seperate part of the data for validation \n",
    "X_org, X_valid,y_org, y_valid = train_test_split(X, y, test_size=0.2, random_state=80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be44d44",
   "metadata": {},
   "source": [
    "## Model trainin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4189c3",
   "metadata": {},
   "source": [
    "### Hyper parameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59363baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "import optuna\n",
    "import optuna.visualization as vis\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# ========== [1] Encode labels ==========\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y_org)\n",
    "\n",
    "# ========== [2] Define Optuna objective functions ==========\n",
    "#A colmplete list of hyper parameters\n",
    "#params = {\n",
    "        #'learning_rate': trial.suggest_float('learning_rate', 0.001, 0.3, log=True),\n",
    "       # 'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "#        'n_estimators': trial.suggest_int('n_estimators', 5, 350),\n",
    "        #'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "       # 'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "       # 'gamma': trial.suggest_float('gamma', 0, 1),\n",
    "        #'reg_alpha': trial.suggest_float('reg_alpha', 0, 1),\n",
    "#        'reg_lambda': trial.suggest_float('reg_lambda', 1, 10),\n",
    "       #'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "        #'use_label_encoder': False,\n",
    "        #'eval_metric': 'mlogloss'\n",
    "#    }\n",
    "\n",
    "def objective_xgb(trial):\n",
    "    params = {\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.001, 0.3, log=True),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 300),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 1, 10),\n",
    "        'use_label_encoder': False,\n",
    "        'eval_metric': 'mlogloss'\n",
    "    }\n",
    "\n",
    "    trial.set_user_attr(\"model_type\", \"xgb\")\n",
    "    model = XGBClassifier(**params)\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    score = cross_val_score(model, X_org, y_encoded, cv=cv, scoring='accuracy').mean()\n",
    "    return score\n",
    "\n",
    "\n",
    "def objective_rf(trial):\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 300),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 20),\n",
    "        'min_samples_split': trial.suggest_int('min_samples_split', 2, 10),\n",
    "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10),\n",
    "        'max_features': trial.suggest_categorical('max_features', [None, 'sqrt', 'log2'])\n",
    "    }\n",
    "\n",
    "    trial.set_user_attr(\"model_type\", \"rf\")\n",
    "    model = RandomForestClassifier(**params, random_state=42)\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    score = cross_val_score(model, X_org, y_encoded, cv=cv, scoring='accuracy').mean()\n",
    "    return score\n",
    "\n",
    "# ========== [3] Run the Optuna study ==========\n",
    "# Choose which model to tune use xgb or rf (random forest)\n",
    "use_model = \"xgb\" # \"rf\" # \n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "\n",
    "if use_model == \"xgb\":\n",
    "    study.optimize(objective_xgb, n_trials=50)\n",
    "else:\n",
    "    study.optimize(objective_rf, n_trials=50)\n",
    "\n",
    "# ========== [4] Train the final model ==========\n",
    "print(\"Best Accuracy:\", study.best_value)\n",
    "print(\"Best Parameters:\", study.best_params)\n",
    "\n",
    "model_type = study.best_trial.user_attrs.get(\"model_type\")\n",
    "\n",
    "if model_type == \"xgb\":\n",
    "    final_model = XGBClassifier(**study.best_params)\n",
    "elif model_type == \"rf\":\n",
    "    final_model = RandomForestClassifier(**study.best_params, random_state=42)\n",
    "else:\n",
    "    raise ValueError(\"Unknown model type in study.\")\n",
    "\n",
    "final_model.fit(X_org, y_encoded)\n",
    "\n",
    "# ========== [5] Save model and label encoder ==========\n",
    "joblib.dump(final_model, f\"best_{model_type}_model.joblib\")\n",
    "joblib.dump(label_encoder, 'label_encoder.joblib')\n",
    "\n",
    "# ========== [6] Visualize tuning results ==========\n",
    "vis.plot_optimization_history(study).show()\n",
    "vis.plot_param_importances(study).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb37a2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best Accuracy:\", study.best_value)\n",
    "print(\"Best Parameters:\", study.best_params)\n",
    "\n",
    "# [5] Train final model using best hyperparameters\n",
    "best_params = study.best_params\n",
    "best_params['use_label_encoder'] = False\n",
    "best_params['eval_metric'] = 'mlogloss'\n",
    "\n",
    "final_model = XGBClassifier(**study.best_params, random_state=80)\n",
    "final_model.fit(X_org, y_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078fb444",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Step 1: Make predictions using the best model\n",
    "y_pred = final_model.predict(X_valid)\n",
    "y_valid_encoded=label_encoder.fit_transform(y_valid)\n",
    "\n",
    "# Step 2: Calculate overall accuracy\n",
    "overall_accuracy = accuracy_score(y_valid_encoded, y_pred)\n",
    "print(f\"Overall Accuracy: {overall_accuracy:.2f}\")\n",
    "\n",
    "# Step 3: Detailed classification report for per-label accuracy (precision, recall, F1-score for each label)\n",
    "classification_report_dict = classification_report(y_valid_encoded, y_pred, output_dict=True)\n",
    "classification_report_df = pd.DataFrame(classification_report_dict).transpose()\n",
    "\n",
    "# Display per-label accuracy (recall for each label)\n",
    "print(\"\\nClassification Report:\\n\")\n",
    "print(classification_report_df)\n",
    "\n",
    "# Plot per-label accuracy (recall)\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=classification_report_df.index[:-3], y=classification_report_df['recall'][:-3], palette=\"viridis\")\n",
    "plt.title(\"Per-Label Accuracy (Recall)\")\n",
    "plt.xlabel(\"Labels\")\n",
    "plt.ylabel(\"Recall\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.show()\n",
    "\n",
    "# Step 4: Confusion Matrix\n",
    "cm = confusion_matrix(y_valid_encoded, y_pred, labels=final_model.classes_)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "# Step 5: Feature Importance (Top 10)\n",
    "# Get feature importances from the best model\n",
    "feature_importances = final_model.feature_importances_\n",
    "features = X_org.columns  # Assuming `X_train` has the original features\n",
    "importance_df = pd.DataFrame({'Feature': features, 'Importance': feature_importances})\n",
    "\n",
    "# Sort by importance and select the top 10 features\n",
    "top_10_importances = importance_df.sort_values(by='Importance', ascending=False).head(10)\n",
    "\n",
    "# Plot the top 10 feature importances\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Importance', y='Feature', data=top_10_importances, palette=\"viridis\")\n",
    "plt.title(\"Top 10 Feature Importances\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
